{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Part 1\n",
    "\n",
    "What's the plan?\n",
    "\n",
    "1 Understand our data better in **Exploratory Data Analysis**, do necessary data wrangling\n",
    "\n",
    "2 Use sales from Oct 2015 as predictions for Nov 2015(**Previous Value Benchmark**)\n",
    "\n",
    "3 **Quick Baseline**. Apply some variant of decision tree(without any feature engineering, compare this with previous value benchmark)\n",
    "\n",
    "4 Set up **Cross Validation** to try out different feature engineering ideas\n",
    "\n",
    "5 Tune decision tree models, try to tune and get several diverse models with similar performance\n",
    "\n",
    "6 Use Ensemble methods to boost score\n",
    "  \n",
    "Btw, I'll omit the ploting part of EDA and all outputs of my code, because I am just compiling my notebooks and upload to kaggle as a kernel for future reference. But feel free to use my code here to get started and try my feature engineering ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "593876eb61eef4c5dc8f1ccc118496be7c427a0e"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "acea7ea2efa32fd41d9133669421522935e2b213",
    "execution": {
     "iopub.execute_input": "2022-01-13T16:55:14.804830Z",
     "iopub.status.busy": "2022-01-13T16:55:14.804112Z",
     "iopub.status.idle": "2022-01-13T16:55:16.076918Z",
     "shell.execute_reply": "2022-01-13T16:55:16.075784Z",
     "shell.execute_reply.started": "2022-01-13T16:55:14.804770Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import loadtxt\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from xgboost import plot_tree\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "kernel_with_output = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d87fd338b141c70ce7341e93c95ad4ded7ffdbd"
   },
   "source": [
    "## Data loading\n",
    "Load all provided datasets and get a feel of the data provided to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "8903d7855924feb2d4c0b5373dc1a17651bc0d10",
    "execution": {
     "iopub.execute_input": "2022-01-13T16:55:58.630789Z",
     "iopub.status.busy": "2022-01-13T16:55:58.630038Z",
     "iopub.status.idle": "2022-01-13T16:55:58.673420Z",
     "shell.execute_reply": "2022-01-13T16:55:58.671875Z",
     "shell.execute_reply.started": "2022-01-13T16:55:58.630726Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    sales_train = pd.read_csv('data/sales_train.csv')\n",
    "    items = pd.read_csv('data/items.csv')\n",
    "    shops = pd.read_csv('data/shops.csv')\n",
    "    item_categories = pd.read_csv('data/item_categories.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea523489895e88e92ebbc54934f50f3ace0ae38a"
   },
   "source": [
    "## Insert missing rows and aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fc37a4d4d3194efe0ab8e59a59e023bab7f05557",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.351606Z",
     "iopub.status.idle": "2022-01-13T16:55:16.352436Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = []\n",
    "    for block_num in sales_train['date_block_num'].unique():\n",
    "        cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n",
    "        cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n",
    "        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "    index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "    # Aggregations\n",
    "    sales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\n",
    "    groups = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])\n",
    "    trainset = groups.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\n",
    "    trainset = trainset.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\n",
    "    trainset['item_cnt_month'] = trainset['item_cnt_month'].clip(0,20)\n",
    "\n",
    "    trainset = pd.merge(grid,trainset,how='left',on=index_cols)\n",
    "    trainset.item_cnt_month = trainset.item_cnt_month.fillna(0)\n",
    "\n",
    "    # Get category id\n",
    "    trainset = pd.merge(trainset, items[['item_id', 'item_category_id']], on = 'item_id')\n",
    "    trainset.to_csv('trainset_with_grid.csv')\n",
    "\n",
    "    trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e034a42fdac02318f5a7de045f3f94746fcaa6cd"
   },
   "source": [
    "# Previous Value Benchmark\n",
    "**Copy from coursera**  \n",
    "\"\n",
    "A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.\n",
    "\n",
    "Generating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.\"\n",
    "\n",
    "** Comments **\n",
    "\n",
    "Simply put: Use October 2015 sales(number of items sold) as our predictions for sales of November 2015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f7bd2b50fc6a90dacdc694b36b14d2f1f431f1a8",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.353499Z",
     "iopub.status.idle": "2022-01-13T16:55:16.354314Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'month'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6852/630857280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkernel_with_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprev_month_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2015\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_month_selector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shop_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item_cnt_month'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'shop_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'item_cnt_month'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'month'"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    prev_month_selector = (trainset.month == 10) & (trainset.year == 2015)\n",
    "    train_subset = trainset[prev_month_selector]\n",
    "    groups = train_subset[['shop_id', 'item_id', 'item_cnt_month']].groupby(by = ['shop_id', 'item_id'])\n",
    "    train_subset = groups.agg({'item_cnt_month':'sum'}).reset_index()\n",
    "    train_subset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93c704384588439502957d718f4215bc16735da8",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.355829Z",
     "iopub.status.idle": "2022-01-13T16:55:16.357973Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    merged = test.merge(train_subset, on=[\"shop_id\", \"item_id\"], how=\"left\")[[\"ID\", \"item_cnt_month\"]]\n",
    "    merged.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28353cef5e25a434fe25d1b1fe5e5bbc2c1323e2"
   },
   "source": [
    "After merging, we will have lots of missing values of item_cnt_month. This is because we only have so much shop_id/item_id pair from Oct 2015. Fill missing values with 0 and clip values to range (0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48ec6947c381bde56835b77fd96f678ca88b6d04",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.359681Z",
     "iopub.status.idle": "2022-01-13T16:55:16.361359Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    merged['item_cnt_month'] = merged.item_cnt_month.fillna(0).clip(0,20)\n",
    "    submission = merged.set_index('ID')\n",
    "    submission.to_csv('benchmark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "591b47e0c12c595a581e3f960d297be982e5d692"
   },
   "source": [
    "# Quick Baseline with XGBoost\n",
    "Here, I'll use only the following features to make a quick baseline solution for the problem  \n",
    "  \n",
    "  **'shop_id', 'item_id', 'item_category_id', 'date_block_num'**  \n",
    "  \n",
    "Note that target is **item_cnt_month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "a085e0a332b42bac116270a2437d817f364dbb37",
    "execution": {
     "iopub.execute_input": "2022-01-13T16:56:24.926876Z",
     "iopub.status.busy": "2022-01-13T16:56:24.926502Z",
     "iopub.status.idle": "2022-01-13T16:56:24.948160Z",
     "shell.execute_reply": "2022-01-13T16:56:24.946030Z",
     "shell.execute_reply.started": "2022-01-13T16:56:24.926822Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # Extract features and target we want\n",
    "    baseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_cnt_month']\n",
    "    train = trainset[baseline_features]\n",
    "    # Remove pandas index column\n",
    "    train = train.set_index('shop_id')\n",
    "    train.item_cnt_month = train.item_cnt_month.astype(int)\n",
    "    train['item_cnt_month'] = train.item_cnt_month.fillna(0).clip(0,20)\n",
    "    # Save train set to file\n",
    "    train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "60b6c336c90d533fb1f410a568a1008f050fd156",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.366469Z",
     "iopub.status.idle": "2022-01-13T16:55:16.368100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    dataset = loadtxt('train.csv', delimiter=\",\" ,skiprows=1, dtype = int)\n",
    "    trainx = dataset[:, 0:4]\n",
    "    trainy = dataset[:, 4]\n",
    "\n",
    "    test_dataset = loadtxt('data/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\n",
    "    test_df = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n",
    "\n",
    "    # Make test_dataset pandas data frame, add category id and date block num, then convert back to numpy array and predict\n",
    "    merged_test = pd.merge(test_df, items, on = ['item_id'])[['shop_id','item_id','item_category_id']]\n",
    "    merged_test['date_block_num'] = 33\n",
    "    merged_test.set_index('shop_id')\n",
    "    merged_test.head(3)\n",
    "\n",
    "    model = xgb.XGBRegressor(max_depth = 10, min_child_weight=0.5, subsample = 1, eta = 0.3, num_round = 1000, seed = 1)\n",
    "    model.fit(trainx, trainy, eval_metric='rmse')\n",
    "    preds = model.predict(merged_test.values)\n",
    "\n",
    "    df = pd.DataFrame(preds, columns = ['item_cnt_month'])\n",
    "    df['ID'] = df.index\n",
    "    df = df.set_index('ID')\n",
    "    df.to_csv('simple_xgb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64677c276f34d62b9278838a93f4caa22c427f88"
   },
   "source": [
    "After my first submission to Kaggle, I get RMSE score of about 15. Definitely not acceptable. After clipping the target to range 0-20, I got RMSE of 2.12. Which is close to what I expect a plain Gradient Boosted Tree can get. In order to improve the score, we'll set up a cross validation scheme below and try different feature engineering ideas and see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfd2569538bdcf1c17b7500a530e3910e3fa755c"
   },
   "source": [
    "# Part2\n",
    "## Set up some global vars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "16a33a52e589a46038290776e285307352d6893b",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.369643Z",
     "iopub.status.idle": "2022-01-13T16:55:16.371287Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    # Set seeds and options\n",
    "    np.random.seed(10)\n",
    "    pd.set_option('display.max_rows', 231)\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "\n",
    "    # Feature engineering list\n",
    "    new_features = []\n",
    "    enable_feature_idea = [True, True, True, True, True, True, True, True, True, True]\n",
    "\n",
    "    # Some parameters(maybe add more periods, score will be better) [1,2,3,12]\n",
    "    lookback_range = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "    tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e3ac3b16ed5614d255cc89376c9477a21d366a3"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d4b24b22de52b4b034dc58a7c682fe0199b38b73",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.372875Z",
     "iopub.status.idle": "2022-01-13T16:55:16.374885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test set...\n",
      "Merging with other datasets...\n",
      "Took 4 seconds to train and predict val set\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    current = time.time()\n",
    "\n",
    "    trainset = pd.read_csv('trainset_with_grid.csv')\n",
    "    items = pd.read_csv('data/items.csv')\n",
    "    shops = pd.read_csv('data/shops.csv')\n",
    "\n",
    "\n",
    "    # Only use more recent data\n",
    "    start_month = 0\n",
    "    end_month = 33\n",
    "    trainset = trainset[['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_price', 'item_cnt_month']]\n",
    "    trainset = trainset[(trainset.date_block_num >= start_month) & (trainset.date_block_num <= end_month)]\n",
    "\n",
    "    print('Loading test set...')\n",
    "    test_dataset = loadtxt('data/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\n",
    "    testset = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n",
    "\n",
    "    print('Merging with other datasets...')\n",
    "    # Get item category id into test_df\n",
    "    testset = testset.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\n",
    "    testset['date_block_num'] = 34\n",
    "    # Make testset contains same column as trainset so we can concatenate them row-wise\n",
    "    testset['item_cnt_month'] = -1\n",
    "\n",
    "    train_test_set = pd.concat([trainset, testset], axis = 0) \n",
    "\n",
    "    end = time.time()\n",
    "    diff = end - current\n",
    "    print('Took ' + str(int(diff)) + ' seconds to train and predict val set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2737ba27e7868bfe7cdb8685b4502fa6f5811dde"
   },
   "source": [
    "## Fix category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ebd0763a93b9e9b39f193160a499bf30d65bf4b7",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.376640Z",
     "iopub.status.idle": "2022-01-13T16:55:16.378482Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    item_cat = pd.read_csv('data/item_categories.csv')\n",
    "\n",
    "    # Fix category\n",
    "    l_cat = list(item_cat.item_category_name)\n",
    "    for ind in range(0,1):\n",
    "        l_cat[ind] = 'PC Headsets / Headphones'\n",
    "    for ind in range(1,8):\n",
    "        l_cat[ind] = 'Access'\n",
    "    l_cat[8] = 'Tickets (figure)'\n",
    "    l_cat[9] = 'Delivery of goods'\n",
    "    for ind in range(10,18):\n",
    "        l_cat[ind] = 'Consoles'\n",
    "    for ind in range(18,25):\n",
    "        l_cat[ind] = 'Consoles Games'\n",
    "    l_cat[25] = 'Accessories for games'\n",
    "    for ind in range(26,28):\n",
    "        l_cat[ind] = 'phone games'\n",
    "    for ind in range(28,32):\n",
    "        l_cat[ind] = 'CD games'\n",
    "    for ind in range(32,37):\n",
    "        l_cat[ind] = 'Card'\n",
    "    for ind in range(37,43):\n",
    "        l_cat[ind] = 'Movie'\n",
    "    for ind in range(43,55):\n",
    "        l_cat[ind] = 'Books'\n",
    "    for ind in range(55,61):\n",
    "        l_cat[ind] = 'Music'\n",
    "    for ind in range(61,73):\n",
    "        l_cat[ind] = 'Gifts'\n",
    "    for ind in range(73,79):\n",
    "        l_cat[ind] = 'Soft'\n",
    "    for ind in range(79,81):\n",
    "        l_cat[ind] = 'Office'\n",
    "    for ind in range(81,83):\n",
    "        l_cat[ind] = 'Clean'\n",
    "    l_cat[83] = 'Elements of a food'\n",
    "\n",
    "    lb = preprocessing.LabelEncoder()\n",
    "    item_cat['item_category_id_fix'] = lb.fit_transform(l_cat)\n",
    "    item_cat['item_category_name_fix'] = l_cat\n",
    "    train_test_set = train_test_set.merge(item_cat[['item_category_id', 'item_category_id_fix']], on = 'item_category_id', how = 'left')\n",
    "    _ = train_test_set.drop(['item_category_id'],axis=1, inplace=True)\n",
    "    train_test_set.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n",
    "\n",
    "    _ = item_cat.drop(['item_category_id'],axis=1, inplace=True)\n",
    "    _ = item_cat.drop(['item_category_name'],axis=1, inplace=True)\n",
    "\n",
    "    item_cat.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n",
    "    item_cat.rename(columns = {'item_category_name_fix':'item_category_name'}, inplace = True)\n",
    "    item_cat = item_cat.drop_duplicates()\n",
    "    item_cat.index = np.arange(0, len(item_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4142355194b030ae6e4d4a62e2ce1fea6935cf82"
   },
   "source": [
    "# Idea 0: Add previous shop/item sales as feature (Lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "eb5eaa3887bac2c8e3934e40f62ede2fdbd1afb8",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.380221Z",
     "iopub.status.idle": "2022-01-13T16:55:16.382065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:02<00:00,  5.18s/it]\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[0]:\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_shopitem_sales_' + str(diff)\n",
    "            trainset2 = train_test_set.copy()\n",
    "            trainset2.loc[:, 'date_block_num'] += diff\n",
    "            trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13e87f447a3d262ae8690918f827e59e9e262819"
   },
   "source": [
    "# Idea 1: Add previous item sales as feature (Lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0ee41b981d34911461476da515ff2379d85113f7",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.383752Z",
     "iopub.status.idle": "2022-01-13T16:55:16.384746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:27<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[1]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_item_sales_' + str(diff)\n",
    "            result = groups.agg({'item_cnt_month':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9a874dad4a304013a4aec6d32be8f9a9e09dc4"
   },
   "source": [
    "# Idea 2: Add previous shop/item price as feature (Lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c05be3cb450b3f0a4bdd2a4f1570e369c783daf6",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.385794Z",
     "iopub.status.idle": "2022-01-13T16:55:16.386631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:33<00:00,  7.82s/it]\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[3]:\n",
    "        groups = train_test_set.groupby(by = ['shop_id', 'item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_shopitem_price_' + str(diff)\n",
    "            result = groups.agg({'item_price':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_price': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name]\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d630fd1b36c50da83d61d3dbe1f441bd6f9943e6"
   },
   "source": [
    "# Idea 3: Add previous item price as feature (Lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "d3a188b1a9228b1f07b689ade235e200fdbfa5e1",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.387647Z",
     "iopub.status.idle": "2022-01-13T16:55:16.388458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:48<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    if enable_feature_idea[3]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = 'prev_item_price_' + str(diff)\n",
    "            result = groups.agg({'item_price':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_price': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name]\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dd6fb36432c3a3a261d1738ad0da681cf95d65e"
   },
   "source": [
    "# Idea 4: Mean encodings for shop/item pairs(Mean encoding, doesnt work for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a8a92fc73bfee328154f27053db08e213ea1e85e",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.389458Z",
     "iopub.status.idle": "2022-01-13T16:55:16.390241Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mean_encodings(train_test_set, categorical_var_list, target):\n",
    "    feature_name = \"_\".join(categorical_var_list) + \"_\" + target + \"_mean\"\n",
    "\n",
    "    df = train_test_set.copy()\n",
    "    df1 = df[df.date_block_num <= 32]\n",
    "    df2 = df[df.date_block_num <= 33]\n",
    "    df3 = df[df.date_block_num == 34]\n",
    "\n",
    "    # Extract mean encodings using training data(here we don't use month 33 to avoid data leak on validation)\n",
    "    # If I try to extract mean encodings from all months, then val rmse decreases a tiny bit, but test rmse would increase by 4%\n",
    "    # So this is important\n",
    "    mean_32 = df1[categorical_var_list + [target]].groupby(categorical_var_list, as_index=False)[[target]].mean()\n",
    "    mean_32 = mean_32.rename(columns={target:feature_name})\n",
    "\n",
    "    # Extract mean encodings using all data, this will be applied to test data\n",
    "    mean_33 = df2[categorical_var_list + [target]].groupby(categorical_var_list, as_index=False)[[target]].mean()\n",
    "    mean_33 = mean_33.rename(columns={target:feature_name})\n",
    "\n",
    "    # Apply mean encodings\n",
    "    df2 = df2.merge(mean_32, on = categorical_var_list, how = 'left')\n",
    "    df3 = df3.merge(mean_33, on = categorical_var_list, how = 'left')\n",
    "\n",
    "    # Concatenate\n",
    "    train_test_set = pd.concat([df2, df3], axis = 0)\n",
    "    new_features.append(feature_name)\n",
    "    return train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "45414a9761145946fbfb5a5d3f196272d933e2c6",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.391228Z",
     "iopub.status.idle": "2022-01-13T16:55:16.392032Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    create_mean_encodings(train_test_set, ['shop_id', 'item_id'], 'item_cnt_month')\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0947f10980e5ff3b42685767ab45a68c2e519b0d"
   },
   "source": [
    "# Idea 5: Mean encodings for item (Mean encoding, doesnt work for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "161490603118eee6412dc4976b60a077066f2145",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.393029Z",
     "iopub.status.idle": "2022-01-13T16:55:16.393847Z"
    }
   },
   "outputs": [],
   "source": [
    "if kernel_with_output:\n",
    "    train_test_set = create_mean_encodings(train_test_set, ['item_id'], 'item_cnt_month')\n",
    "    train_test_set.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3464c0c751405cd7f3ffa61410e5f7b7226507cf"
   },
   "source": [
    "# Idea 6: Number of month from last sale of shop/item (Use info from past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "ed61cfeb7168ac0f8f26e690122d1d38cabfa115",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.394834Z",
     "iopub.status.idle": "2022-01-13T16:55:16.395625Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [24:12<00:00, 44.03s/it]\n",
      "  0%|          | 0/11128050 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_is_builtin_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6852/2472934899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#new_features.append(feature_name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last_sale_shop_item'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcreate_last_sale_shop_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mnew_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'last_sale_shop_item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m                     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_is_builtin_func'"
     ]
    }
   ],
   "source": [
    "def create_last_sale_shop_item(row):\n",
    "    for diff in range(1,33+1):\n",
    "        feature_name = '_prev_shopitem_sales_' + str(diff)\n",
    "        if row[feature_name] != 0.0:\n",
    "            return diff\n",
    "    return np.nan\n",
    "\n",
    "if kernel_with_output:\n",
    "    lookback_range = list(range(1, 33 + 1))\n",
    "    if enable_feature_idea[6]:\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = '_prev_shopitem_sales_' + str(diff)\n",
    "            trainset2 = train_test_set.copy()\n",
    "            trainset2.loc[:, 'date_block_num'] += diff\n",
    "            trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            #new_features.append(feature_name)\n",
    "\n",
    "    train_test_set.loc[:, 'last_sale_shop_item'] = train_test_set.progress_apply (lambda row: create_last_sale_shop_item(row),axis=1)\n",
    "    new_features.append('last_sale_shop_item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dcff93eb98178a767dfe08b3ed9f3612cbffc75"
   },
   "source": [
    "# Idea 7: Number of month from last sale of item(Use info from past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "bf86126642ac8f48d69c2a5125638ba6dd6c349f",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.396655Z",
     "iopub.status.idle": "2022-01-13T16:55:16.397471Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/33 [00:12<06:31, 12.25s/it]\u001b[A\n",
      "  6%|▌         | 2/33 [00:36<09:56, 19.24s/it]\u001b[A\n",
      "  9%|▉         | 3/33 [00:47<07:41, 15.39s/it]\u001b[A\n",
      " 12%|█▏        | 4/33 [01:00<07:05, 14.68s/it]\u001b[A\n",
      " 15%|█▌        | 5/33 [01:12<06:18, 13.51s/it]\u001b[A\n",
      " 18%|█▊        | 6/33 [01:24<05:53, 13.09s/it]\u001b[A\n",
      " 21%|██        | 7/33 [01:37<05:43, 13.22s/it]\u001b[A\n",
      " 24%|██▍       | 8/33 [01:49<05:15, 12.64s/it]\u001b[A\n",
      " 27%|██▋       | 9/33 [02:03<05:13, 13.07s/it]\u001b[A\n",
      " 30%|███       | 10/33 [02:21<05:36, 14.61s/it]\u001b[A\n",
      " 33%|███▎      | 11/33 [02:37<05:32, 15.10s/it]\u001b[A\n",
      " 36%|███▋      | 12/33 [02:54<05:29, 15.69s/it]\u001b[A\n",
      " 39%|███▉      | 13/33 [03:11<05:17, 15.88s/it]\u001b[A\n",
      " 42%|████▏     | 14/33 [03:29<05:14, 16.55s/it]\u001b[A\n",
      " 45%|████▌     | 15/33 [03:45<04:56, 16.45s/it]\u001b[A\n",
      " 48%|████▊     | 16/33 [04:02<04:42, 16.63s/it]\u001b[A\n",
      " 52%|█████▏    | 17/33 [04:22<04:40, 17.52s/it]\u001b[A\n",
      " 55%|█████▍    | 18/33 [04:41<04:30, 18.01s/it]\u001b[A\n",
      " 58%|█████▊    | 19/33 [04:58<04:10, 17.89s/it]\u001b[A\n",
      " 61%|██████    | 20/33 [05:19<04:04, 18.83s/it]\u001b[A\n",
      " 64%|██████▎   | 21/33 [05:36<03:39, 18.28s/it]\u001b[A\n",
      " 67%|██████▋   | 22/33 [05:58<03:32, 19.34s/it]\u001b[A\n",
      " 70%|██████▉   | 23/33 [06:21<03:25, 20.52s/it]\u001b[A\n",
      " 73%|███████▎  | 24/33 [06:40<02:58, 19.83s/it]\u001b[A\n",
      " 76%|███████▌  | 25/33 [07:06<02:55, 21.92s/it]\u001b[A\n",
      " 79%|███████▉  | 26/33 [07:31<02:38, 22.69s/it]\u001b[A\n",
      " 82%|████████▏ | 27/33 [07:57<02:22, 23.70s/it]\u001b[A\n",
      " 85%|████████▍ | 28/33 [08:25<02:04, 25.00s/it]\u001b[A\n",
      " 88%|████████▊ | 29/33 [08:53<01:43, 25.91s/it]\u001b[A\n",
      " 91%|█████████ | 30/33 [09:22<01:20, 26.69s/it]\u001b[A\n",
      " 94%|█████████▍| 31/33 [09:49<00:54, 27.03s/it]\u001b[A\n",
      " 97%|█████████▋| 32/33 [10:18<00:27, 27.43s/it]\u001b[A\n",
      "100%|██████████| 33/33 [10:48<00:00, 19.66s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/11128050 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_is_builtin_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6852/3344214588.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mnew_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last_sale_item'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcreate_last_sale_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m                     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_is_builtin_func'"
     ]
    }
   ],
   "source": [
    "def create_last_sale_item(row):\n",
    "    for diff in range(1,33+1):\n",
    "        feature_name = '_prev_item_sales_' + str(diff)\n",
    "        if row[feature_name] != 0.0:\n",
    "            return diff\n",
    "    return np.nan\n",
    "if kernel_with_output:\n",
    "    lookback_range = list(range(1, 33 + 1))\n",
    "    if enable_feature_idea[1]:\n",
    "        groups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n",
    "        for diff in tqdm(lookback_range):\n",
    "            feature_name = '_prev_item_sales_' + str(diff)\n",
    "            result = groups.agg({'item_cnt_month':'mean'})\n",
    "            result = result.reset_index()\n",
    "            result.loc[:, 'date_block_num'] += diff\n",
    "            result.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n",
    "            train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n",
    "            train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n",
    "            new_features.append(feature_name)        \n",
    "    train_test_set.loc[:, 'last_sale_item'] = train_test_set.progress_apply (lambda row: create_last_sale_item(row),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9b1198c05e4a86687c589559c966d6801af2a3e"
   },
   "source": [
    "# Idea 8: Item name (Tfidf text feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b2508fdfb1a4a9b77465157480757e174b7f42f4",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.398440Z",
     "iopub.status.idle": "2022-01-13T16:55:16.399222Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11128050 [11:17<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    items_subset = items[['item_id', 'item_name']]\n",
    "    feature_count = 25\n",
    "    tfidf = TfidfVectorizer(max_features=feature_count)\n",
    "    items_df_item_name_text_features = pd.DataFrame(tfidf.fit_transform(items_subset['item_name']).toarray())\n",
    "\n",
    "    cols = items_df_item_name_text_features.columns\n",
    "    for i in range(feature_count):\n",
    "        feature_name = 'item_name_tfidf_' + str(i)\n",
    "        items_subset[feature_name] = items_df_item_name_text_features[cols[i]]\n",
    "        new_features.append(feature_name)\n",
    "\n",
    "    items_subset.drop('item_name', axis = 1, inplace = True)\n",
    "    train_test_set = train_test_set.merge(items_subset, on = 'item_id', how = 'left')\n",
    "    train_test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87511f885a323923e4aa92668b27bf34ec4bd29b"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "75b8599039a9ef057f951f6d2ee1f4251d4355b3",
    "execution": {
     "iopub.status.busy": "2022-01-13T16:55:16.400243Z",
     "iopub.status.idle": "2022-01-13T16:55:16.401036Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.8 GiB for an array with shape (142, 11128050) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6852/4273794699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Retrieve rows for train set, val set, test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mcv_trainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_time_range_lo\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mtrain_time_range_hi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mcv_valset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalidation_time\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcv_trainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv_trainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbaseline_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3447\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3502\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3503\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3504\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3626\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3627\u001b[0m         \"\"\"\n\u001b[1;32m-> 3628\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3629\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3611\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3613\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3615\u001b[0m         new_data = self._mgr.take(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5563\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5565\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5552\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5553\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5555\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5562\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5563\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5565\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1972\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1974\u001b[1;33m         merged_blocks = _merge_blocks(\n\u001b[0m\u001b[0;32m   1975\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1976\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.8 GiB for an array with shape (142, 11128050) and data type float64"
     ]
    }
   ],
   "source": [
    "if kernel_with_output:\n",
    "    current = time.time()\n",
    "\n",
    "    baseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num'] +  new_features + ['item_cnt_month']\n",
    "\n",
    "    # Clipping to range 0-20\n",
    "    train_test_set['item_cnt_month'] = train_test_set.item_cnt_month.fillna(0).clip(0,20)\n",
    "\n",
    "    # train: want rows with date_block_num from 0 to 31\n",
    "    train_time_range_lo = (train_test_set['date_block_num'] >= 0)\n",
    "    train_time_range_hi =  (train_test_set['date_block_num'] <= 32)\n",
    "\n",
    "    # val: want rows with date_block_num from 22\n",
    "    validation_time =  (train_test_set['date_block_num'] == 33)\n",
    "\n",
    "    # test: want rows with date_block_num from 34\n",
    "    test_time =  (train_test_set['date_block_num'] == 34)\n",
    "\n",
    "\n",
    "    # Retrieve rows for train set, val set, test set\n",
    "    cv_trainset = train_test_set[train_time_range_lo & train_time_range_hi]\n",
    "    cv_valset = train_test_set[validation_time]\n",
    "    cv_trainset = cv_trainset[baseline_features]\n",
    "    cv_valset = cv_valset[baseline_features]\n",
    "    testset = train_test_set[test_time]\n",
    "    testset = testset[baseline_features]\n",
    "\n",
    "    # Prepare numpy arrays for training/val/test\n",
    "    cv_trainset_vals = cv_trainset.values.astype(int)\n",
    "    trainx = cv_trainset_vals[:, 0:len(baseline_features) - 1]\n",
    "    trainy = cv_trainset_vals[:, len(baseline_features) - 1]\n",
    "\n",
    "    cv_valset_vals = cv_valset.values.astype(int)\n",
    "    valx = cv_valset_vals[:, 0:len(baseline_features) - 1]\n",
    "    valy = cv_valset_vals[:, len(baseline_features) - 1]\n",
    "\n",
    "    testset_vals = testset.values.astype(int)\n",
    "    testx = testset_vals[:, 0:len(baseline_features) - 1]\n",
    "\n",
    "    print('Fitting...')\n",
    "    model = xgb.XGBRegressor(max_depth = 11, min_child_weight=0.5, subsample = 1, eta = 0.3, num_round = 1000, seed = 1, nthread = 16)\n",
    "    model.fit(trainx, trainy, eval_metric='rmse')\n",
    "\n",
    "\n",
    "    preds = model.predict(valx)\n",
    "    # Clipping to range 0-20\n",
    "    preds = np.clip(preds, 0,20)\n",
    "    print('val set rmse: ', sqrt(mean_squared_error(valy, preds)))\n",
    "\n",
    "    preds = model.predict(testx)\n",
    "    # Clipping to range 0-20\n",
    "    preds = np.clip(preds, 0,20)\n",
    "    df = pd.DataFrame(preds, columns = ['item_cnt_month'])\n",
    "    df['ID'] = df.index\n",
    "    df = df.set_index('ID')\n",
    "    df.to_csv('test_preds.csv')\n",
    "    print('test predictions written to file')\n",
    "\n",
    "    end = time.time()\n",
    "    diff = end - current\n",
    "    print('Took ' + str(int(diff)) + ' seconds to train and predict val, test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4908eb443addb3140d8541c6ac863a37ebc6a44d"
   },
   "source": [
    "# Model Ensemble: Stacking\n",
    "\n",
    "I have tried to combine models from CatBoost, XGboost and LightGBM with stacking, but the results aren't as good as using XGboost alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eeb8b7bd57b93ffb4319f2949556393bf51c525"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In the end, I got a rmse score of 0.89874 in the public leader board(top 11). For the private grader, I got rmse score of 0.88(yeah!! no overfitting)\n",
    "\n",
    "I learned one most important thing in this competition. Feature engineering is the single most important thing in machine learning! If you don't expose the interactions of data to your model explictily, then no matter how you tune your model, it can not learn those interactions between data!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129a733e",
   "metadata": {},
   "source": [
    "# Creating a Top 10 recommendations dataframe\n",
    "\n",
    "Using the KNNBaseline algorithm, this notebook calculates all the predictions with the data for users with ratings 151-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05d025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import surprise as sp\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8455e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "books = pd.read_csv('../data/processed/books.csv')\n",
    "ratings = pd.read_csv('../data/raw/ratings.csv')\n",
    "\n",
    "# load the models\n",
    "SVDpp = pickle.load(open('../models/SVDpp_150.sav', 'rb'))\n",
    "KNNBaseline = pickle.load(open('../models/KNNBaseline_150.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd1abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 999 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_data_surprise (df, minstar=1, maxstar=3, col1='user_id', col2='route', col3='rating'):\n",
    "    '''\n",
    "    Produces a surpise library data object from original dataframe\n",
    "\n",
    "    ---Parameters---\n",
    "\n",
    "    df (Pandas DataFrame)\n",
    "    minstar (int) minimum rating possible in dataset (default set to 1)\n",
    "    maxstar (int) maximum rating possible in dataset (default set to 5)\n",
    "    col1 (string) column name that MUST correspond the the users in the df\n",
    "    col2 (string) column name that MUST corresponds the the items in the df\n",
    "    col3 (string) column name that corresponds the the ratings of the items in the df\n",
    "\n",
    "    ---Returns---\n",
    "    surprise library data object to manipulate later\n",
    "\n",
    "    '''\n",
    "    # need to specify the rating_scale of stars (default 1-3 stars)\n",
    "    reader = sp.Reader(rating_scale=(minstar, maxstar))\n",
    "    # The columns must correspond to user id, item id and ratings (in that order).\n",
    "    data = sp.Dataset.load_from_df(df[[col1, col2, col3]], reader)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2d5fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e4fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def top_ten_df (df, algo_name='SVDpp'):\n",
    "    '''\n",
    "    inputs:\n",
    "    df (Pandas DF) the dataframe that you would like to train on\n",
    "\n",
    "    outputs:\n",
    "    top_ten_df (DataFrame Pandas) returns a dataframe with the top ten predictions for every user in your original dataframe\n",
    "    '''\n",
    "    \n",
    "    reader = sp.Reader(rating_scale=(1, 5))\n",
    "    data = sp.Dataset.load_from_df(df[['user_id', 'book_id', 'rating']], reader)\n",
    "\n",
    "    # First train an KNN Baseline algorithm on dataset\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    if algo_name == 'SVDpp': \n",
    "        algo = SVDpp\n",
    "    else: \n",
    "        algo = KNNBaseline\n",
    "                    \n",
    "    # algo = sp.KNNBaseline() # n_epochs= 18, lr_all= 0.01, reg_all= 0.175\n",
    "    # algo.fit(trainset)\n",
    "\n",
    "    # Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "    testset = trainset.build_anti_testset()# THIS TAKES THE MOST RAM\n",
    "    predictions = algo.test(testset)\n",
    "    sp.accuracy.rmse(predictions)\n",
    "\n",
    "    #create a dictionary of predictions\n",
    "    top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "    #Turn the dictionary into a df\n",
    "    top_ten_df = pd.DataFrame(top_n)\n",
    "\n",
    "    return top_ten_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f78ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_ratings = ratings.user_id.value_counts()\n",
    "hundred_fifty_ids = num_ratings[num_ratings.values > 150].index\n",
    "hundred_fifty = ratings[ratings.user_id.isin(hundred_fifty_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f0141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def df_samp_unique_vals (df, percent, col1, col2=None):\n",
    "    '''\n",
    "    Takes a random sample of current dataframe while keeping a few column values unique to decrease matrix sparsity of sample\n",
    "\n",
    "    ---Parameters---\n",
    "    df (Pandas DataFrame)\n",
    "    percent (float) enter a decimal of the percent sample you want\n",
    "    col1 (\"string\") column name you want to keep retain unique values for (include quotation marks)\n",
    "    col2 (\"string\") column name you want to keep retain unique values for (include quotation marks)\n",
    "\n",
    "    ---Return---\n",
    "    matrix stats of new df\n",
    "    df_samp (Pandas DataFrame) as a percent sample of the original while keeping the columns entered unique\n",
    "    '''\n",
    "    x = df.book_id.nunique()\n",
    "    y = df.user_id.nunique()\n",
    "    print(f\"Initial num of unique books: {x}\")\n",
    "    print(f\"Initial num of unique users: {y}\")\n",
    "    print(f\"Matrix size: {x*y}\")\n",
    "    print(f\"Shape of df: {df.shape}\")\n",
    "    print(f\"Density of matrix: {(df.shape[0])/(x*y)}\")\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # df.user_id.unique().sample(frac= percent) #(more efficient code to explore??)\n",
    "    df_drop = df.drop_duplicates(subset=[col1])\n",
    "    print (f\"User drop: {len(df_drop)}\")\n",
    "    if col2:\n",
    "        df_drop = df_drop.drop_duplicates(subset=[col2])\n",
    "        print (f\"Book drop: {len(df_drop)}\")\n",
    "    #take a sample of the unique values\n",
    "    sample1 = df_drop.sample(frac= percent, random_state=101)#Random state = random seed for .sample\n",
    "    print (f\"length of entire sample w/ unique users & books: {len(sample1)}\")\n",
    "\n",
    "    #turn the unique routes & user names into a list to reference\n",
    "    sample1= sample1.loc[:, [col1, col2]].values.T.ravel()\n",
    "    lst1= sample1.tolist()\n",
    "\n",
    "    #Filter out the original DF with only unique the unique values\n",
    "    df_samp = df[(df[col1].isin(lst1)) & (df[col2].isin(lst1))]\n",
    "    \n",
    "    x = df_samp.book_id.nunique()\n",
    "    y = df_samp.user_id.nunique()\n",
    "    print(f\"Final num of unique books: {x}\")\n",
    "    print(f\"Final num of unique users: {y}\")\n",
    "    print(f\"Matrix size: {x*y}\")\n",
    "    print(f\"Shape of df: {df_samp.shape}\")\n",
    "    print(f\"Density of matrix: {(df_samp.shape[0])/(x*y)}\")\n",
    "\n",
    "    return df_samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20987e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial num of unique books: 10000\n",
      "Initial num of unique users: 53424\n",
      "Matrix size: 534240000\n",
      "Shape of df: (5976479, 3)\n",
      "Density of matrix: 0.011186880428271938\n",
      "---------------------\n",
      "User drop: 53424\n",
      "Book drop: 7456\n",
      "length of entire sample w/ unique users & books: 4846\n",
      "Final num of unique books: 5745\n",
      "Final num of unique users: 8757\n",
      "Matrix size: 50308965\n",
      "Shape of df: (687480, 3)\n",
      "Density of matrix: 0.013665158883709892\n",
      "Wall time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create a 65% sample out of the dataframe\n",
    "sample = df_samp_unique_vals(ratings, .65, \"user_id\", \"book_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4a0ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#not needed for below cell\n",
    "reader = sp.Reader(rating_scale=(1, 5))\n",
    "sample_data = sp.Dataset.load_from_df(sample[['user_id','book_id','rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb73b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2981\n",
      "Wall time: 11min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_10_150_ratings = top_ten_df(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1a74bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "top_10_150_ratings.to_csv('../data/processed/top_10_SVDpp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45c6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2630\n",
      "Wall time: 11min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_10_150_ratings = top_ten_df(sample, algo_name='KNNBaseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb35efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "top_10_150_ratings.to_csv('../data/processed/top_10_KNNBaseline.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
